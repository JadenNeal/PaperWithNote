# 策略梯度简介(二)

本篇主要讲了在线策略、离线策略和自动差分器求梯度。

## 在线策略和离线策略算法

首先谈谈在线策略(`on-policy`)。
on-policy，**由于策略梯度求期望的时候必须是从当前的分布上采样才能有无偏性**。所以每次改进策略，都必须重新采样，生成新的样本，丢弃旧的样本。但由于NN每次改变都是一点点，因此这对于数据利用率来说是非常低的。
为了引入`off-policy`，思考下面这个问题：能否不从最新的策略$\pi_{\theta}(\tau)$产生样本，而是只利用来自策略$\pi_{\theta^\prime}(\tau)$的样本？
所以我们能用的就是这个重要性样本(importance sampling)。  
所谓的**重要性抽样**，是一种用于在概率下估计函数期望的方法。公式如下：
$$E_{x\sim p(x)}[f(x)] = \int p(x)f(x)dx$$ $$=\int\frac{q(x)}{q(x)}p(x)f(x)dx = \int q(x)\frac{p(x)}{q(x)}f(x)dx$$ $$= E_{x\sim q(x)}\left[\frac{p(x)}{q(x)}f(x)\right]$$
我们从$\pi_{\theta}$获得$p$，从$\pi_{\theta^\prime}$获得$q$，从而可以写成以下的形式：$$J(\theta) = E_{\tau\sim \pi_{\theta}(\tau)}\left[\frac{\pi_{\theta}(\tau)}{\bar{\pi}_{\theta}(\tau)}\right]r(\tau)$$
因此**离线策略**就是从$\theta^{\prime}$中得到$\theta$。

## 自动差分器求梯度

我们现在想找到一个比较简单的方法去实现策略梯度法，以利用上TensorFlow或者PyTorch的自动差分器。回顾梯度：
$$\nabla_{\theta}J(\theta) = \frac{1}{N}\sum_{i=1}^N\sum_{t=1}^T\left[\nabla_{\theta}\log\pi_{\theta}(a_{i,t}|s_{i,t})Q_{i,t}\right]$$
与之前的处理方法类似，由于很难得到数值解，因此还是采用近似求解的方法。考虑极大似然的目标函数：$$J_{ML}(\theta) \approx \frac{1}{N}\sum_{i=1}^N\sum_{t=1}^T\left[\log\pi_{\theta}(a_{i,t}|s_{i,t})\right]$$
对该目标函数求导，则$$\nabla_{\theta}J_{ML}(\theta) \approx \frac{1}{N}\sum_{i=1}^N\sum_{t=1}^T\left[\nabla_{\theta}\log\pi_{\theta}(a_{i,t}|s_{i,t})\right]$$
对比所要求的结果，只要将$Q_{i,t}$放进去即可。而$Q_{i,t}$是不依赖于参数的，因此只需要再定义一个虚拟的损失函数即可，即
$$\tilde{J}(\theta) \approx \frac{1}{N}\sum_{i=1}^N\sum_{t=1}^T\left[\nabla_{\theta}\log\pi_{\theta}(a_{i,t}|s_{i,t})Q_{i,t}\right]$$
其中权重就是$Q_{i,t}$，然后再利用自动差分器求梯度就行了。
