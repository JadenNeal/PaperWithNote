---
title: 强化学习简介
date: 2019-12-31 19:02:43
tags: 强化学习
---
本节内容：

1. MDP（马尔可夫决策过程）的定义
2. 强化学习问题的定义
3. 详解RL算法
4. RL算法类型概述

<!--more-->

## MDP（马尔科夫决策过程）
### 重要性
可以毫不夸张地说，MDP定义了RL的基本世界观。它有state, action, reward, transition，这些东西恰好就是RL世界的基础。

### 马尔可夫链
马尔可夫链是由状态空间和转移概率组成的。即：$$M = \\{S, T \\}$$
其中\\(S\\)表示**状态**，\\(T\\)表示**转移算子**，**下一状态只与当前状态有关。**

### 完全可观察的MDP
$$M = \\{S, A, T, r\\}$$
其中：

- S：状态空间。离散或者连续的均可
- A：动作空间。离散或者连续的均可
- T：转移算子，是一个**张量**！
- r：激励函数，\\(R ← S \times A\\)

<center><img src="https://s2.ax1x.com/2019/12/31/l3lKqH.jpg" alt="l3lKqH.jpg" border="0" />

这里S和A一起决定了下一状态</center>

### 部分可观察的MDP——POMDP
这是MDP最一般的形式，但是，我们并不经常遇到这种形式。此时，则有：$$M = \\{S, A, O, T, \epsilon, r\\}$$
其中：

- S：状态空间
- A：动作空间
- O：观察空间
- T：转移算子
- \\(\epsilon\\)：输出概率
- r：激励函数，也可以称作是奖励函数

## 强化学习的目标
找到一组参数，能够使得所有动作得到奖励总和的期望值最大。
$$p_{\theta}(s_1, a_1, \dots, s_T, a_T) = p(s_1)\prod_{t=1}^T\pi_{\theta}(a_t|s_t)p(s_{t+1}|s_t, a_t)$$

$$\theta^* = \arg max(\theta)E_{\tau \sim p_{\theta}(\tau)}\left\[\sum_tr(s_t, a_t)\right\]$$
这里之所以要写成期望的形式，是因为在给定参数向量下， 状态和行动仍然是随机变量，因此要求的是期望。

## 强化学习算法
分成三部分：

- 产生样本
- 拟合模型/估计回报（最为耗时）
- 改进策略

如何应对随机系统呢？答案是使用条件概率。

## Q函数和V函数
### Q函数定义
Q函数的定义是状态-动作值函数，是对每一个时间步进行定义。它是一个对特定策略\\(\pi\\)的、表示累计奖励的函数。
$$Q^{\pi}(s_t, a_t) = \sum_{t'=t}^{T}E_{\pi\theta}[r(s_{t'}, a_{t'})|s_t, a_t]$$

### V函数定义
V函数是指值函数，也是表示累计奖励的函数。有以下两种定义方法。

1. \\(V^{\pi}(s_t) = \sum_{t'=t}^{T}E_{\pi\theta}[r(s_{t'}, a_{t'})|s_t]\\)
2. \\(V^{\pi}(s_t) = E_{a_t\sim\pi(a_t|s_t)}[Q^{\pi}(s_t, a_t)]\\)

**上述两种定义是等价的。**

## 算法种类简介
$$\theta^{*} = \arg max(\theta)E_{\tau\sim p_{\theta}(\tau)}\left\[\sum_tr(s_t, a_t)\right\]$$

1. 策略梯度算法（Policy Gradients）：目的是计算上述\\(\theta^*\\)的导数来计算期望。
2. 基于值的算法：直接估计优化策略的Q函数或V函数，并通过\\(\arg max\\)来得到最优解。
3. Actor-critic算法：这是上述二者的结合。估计现有策略的V函数和Q函数，利用它来改进策略。
4. 基于模型的强化学习：估计传递模型，然后可能采取不同的做法，比如：
    - 使用它（指估计出来的模型，下同）来做出计划（不需要显式的策略）
    - 使用它来改进策略
    - 其他

## 为什么有许多RL算法？
1. 不同的折中
    - 不同的样本效率（sample efficiency）。比如对同一算法来说，用100万个样本能得到1000个反馈值，而另一个算法用1000个样本也能达到同一效果，那么就可以说第二种算法更有样本效率。
    - 稳定性和易用性。这两种性质并没有精确的定义。但我们可以用一种方法进行“测量”。运行N次算法，有多大频率能收敛到一个较好的值，需要怎样精心选择超参数来取得好的收敛效果。

2. 不同的假设
    - 随机的或者是确定的。这里可以指动态系统，也可以指策略。
    - 连续的或离散的。
    - 有限期的或无限期的。
3. 不同设置的难易程度  
    这里说的是对于特定的问题，表示策略更为容易，还是表示模型更容易。  
    - 表示策略更容易
    - 表示模型更容易

## 样本效率
**样本效率**：需要多少样本来得到一个好的策略？  
有很多因素影响着算法是否有效，但**是否是离线的算法**是最重要的一个。

**离线策略：**能够用样本来改进策略，而不需要从该策略中生成新的样本。  
**在线策略：**每当策略改变时，哪怕只是一点点，我们都需要生成新的样本。

<div align="center"><img src="https://s2.ax1x.com/2020/01/05/lBHU8H.jpg" alt="lBHU8H.jpg" border="0" /></div>
<center>各种算法样本效率一览</center>

### 为什么要使用低效率算法
继续讨论的一个问题是，根据上面的图片，有些算法样本效率很低，为什么我们还会继续使用呢？  
答案是**高效率并不意味着该算法一定好。**样本效率并不是算法优劣的唯一度量。

算法真正的执行时间，也就是需要等待的时间。当使用一个很简单但可以并行的模拟器，可能样本效率不高，但却缩短了执行时间。这也就是人们仍然在使用进化策略之类的算法的原因。

## 稳定性和易用性
有几个重要的问题： 

1. 稳定性和易用性可以使算法收敛吗？
2. 如果收敛，收敛到什么结果呢？
3. 每次运行都收敛吗？

上面的问题看似来似乎有点莫名其妙，因为在监督学习，或者在非监督学习情况下使用比梯度下降更加规范的算法（比如凸优化）时，我们的目标就是使用梯度下降来得到局部最优。因此上述的问题总是有确切的答案：**会收敛、每次都收敛、收敛到局部最优。**

但是在强化学习领域就有必要来讨论这三个问题了，因为许多RL算法都不是梯度下降的。 这些算法可能需要看起来像梯度下降的式子，但事实上却不是在做梯度下降。比如：

1. Q学习：所求的Q函数并不是在做梯度下降，而是在做固定点循环，来收敛到非常简单的函数类（一般不收敛到复杂的近似函数，比如NN收敛的结果就是复杂的近似函数）。
2. 基于模型的RL：该算法是通过梯度下降去优化模型，但优化模型并不是像优化反馈函数那样，人们通过拟合数据得到最好的模型去预测。**这确实是梯度下降，也是监督学习，但并不是显式地最大化价值函数，而是设法给出一个更精确的模型。**这会带来更好的策略。但这一过程并不确定，因为一些模型错误，可能使得得到反馈函数的代价更加高昂。
3. 策略梯度：它就是梯度下降，但同时也是样本效率最低的。因此当它收敛且收敛到局部最优值时，它需要的样本最多。

**现在正式讨论上述的三个问题**

1. 价值函数拟合：最好的结果：拟合价值函数时，人们最小化的是拟合的错误（也称Bellman error）。也就是输出的价值和估计的价值之间的差距。但是，**缩小二者的差距，并不能保证产生一个好的策略。但如果错误率能降为0，就可以得到一个好的策略。**  
最差的结果，就是并没有优化任何结果。就连许多流行的深度RL价值函数拟合算法都不能保证一定收敛，尤其是在非线性场景中。
2. 基于模型的RL：模型会最小化拟合结果，这可以保证收敛。但是，却不能保证会得到一个更好的策略。
3. 策略梯度：这是唯一一个在实际物体上应用梯度下降（或上升）的算法，这会使得策略梯度算法易用性提高，但也会带来许多问题，比如高方差。

## 不同的假设
有三大假设，这里只列举最普遍的。

1. 假设1：系统是可以全程观察的。这意味着我们可以根据观测或者状态去训练策略。有些假设是不可观察的，那就是符合马尔可夫属性。
    - 一般来说，这种带有欺骗性观测是通过价值函数拟合来假设的。这种方法直接拟合价值函数或者Q函数，隐含假设是有一个\\(Markov\\)状态 。
    - 因此可以通过循环的值函数或者Q函数去迁移这种状态
2. 假设2：情境学习（\\(episodic\quad learning\\)）  
这种情况下学习过程被组织成一个个情景序列。
    - 情境学习是通过直接策略梯度方法来假设。
    - 这种算法是很多基于模型的算法的假设。

3. 假设3：连续性和光滑性
    - 一些连续价值函数学习方法通常是这样假设的；
    - 一些基于模型的RL算法也是如此假设。
