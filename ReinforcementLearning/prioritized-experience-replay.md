# 优先经验回放

## 摘要
**之前工作的问题**：经验过渡是从回放记忆中均匀采样，但这种方法仅仅以同一频率回放过渡，而不管这些经验的重要性。
**本文创新点**：为**优先经验**提出一种框架，对于重要的过渡，回放就较多，因此学习起来更有效率。
**实验过程与结果**：**将优先经验回放应用到DQN上**，获得了超越传统DQN的结果。49个游戏中有41个都超越了，足以说明本文提出的新框架的优越性。

<!--more-->

## 概述
在线RL有两个问题：
1. 相邻更新极度相关，基于梯度的随机算法不再适用。
2. 一些可能有用的罕见经验会被快速丢弃。

因此提出了经验回放（`Experience Replay`），将经验存储在回放记忆中，有利于打断**相关性**，从而减少了需要学习的经验，用更多的计算和记忆来代替。

本论文就是研究了**优先性**对**经验回放**所造成的影响，用TD误差来衡量优劣。由于优先性会导致多种误差，因此该论文又引入了随机优先性，加入偏差，用重要性采样来修正。

## 算法设计
经验回放设计有两种选择：1、存储哪种经验；**2、回放哪种经验**。本论文解决的是后者。

### TD误差的优先级
**经验回放**的核心部分是每次过渡的重要性的衡量标准，一个理想的标准就是RL智能体从当前状态的过渡中学习到的知识数量。然而这种方法不能直接使用，**必须用TD误差来代替**。
优点：对于那种递增式的、在线的RL算法非常有效，比如`SARSA`，`Q-learning`
缺点：对于噪声很大的奖励则效果很差。

### 随机优先性
TD误差优先有一系列的问题：
1. 为了避免对整个记忆扫描，TD误差只会更新回放过的过渡，造成的结果就是初次有着低TD误差的过渡就不会被重放太长时间。
2. 对噪声敏感（比如当奖励是随机的时候），并且会随着`bootstraping`恶化，其中逼近误差就像来自另外一个噪声源。
3. 它只对经验的一小个子集操作——误差缓慢下降（特别是做函数逼近的时候），意味着初始的高误差过渡重放频繁，这就造成了多样性缺失，即同一性太强，容易造成模型过拟合。

为了克服这些问题，论文中引入了一种随机抽样方法，插入在纯贪婪排序与均匀随机抽样中。作者还确保了采样可能性是单调的。

### 偏差退火
优先回放引入了偏差，由于其改变了分布，从而也改变了估计收敛的方法（即使策略和状态分布都是固定的）。论文用下式更正了此偏差：
$$w_i = \left(\frac{1}{N}·\frac{1}{P(i)}\right)^\beta$$ 其中$\beta$称为**重要性权重（IS）**。
若$\beta=1$，就能补偿非均匀概率$P(i)$，当然为了稳定性，也可以用$1/\max_iw_i$来正则化。

论文假设小的偏差可以忽略，于是通过改变$\beta$，来探索重要性采样更正的退火数量的灵活性。
重要性采样（IS）的优点不止于此，由于一阶梯度逼近只是局部可靠，对于大的步长则是毁灭性的结果，因此必须用较小的步长来避免这种结果。论文结果证明，**优先性**确保高误差的过渡会使用多次，尽管IS纠正减少了梯度空间，使得算法可以沿着非线性优化最快的方向进行。

## Atari实验
### 实验简介
环境：Atari游戏
游戏挑战：延迟积分分配、部分可观察的环境、较为困难的函数逼近
baseline：DQN、Double DQN，均使用均匀经验回放
### 实验步骤
1. 对于Baseline，使用理想的神经网络架构、学习算法、回放记忆和估计设置。**唯一的不同是采样机制**，使用的是改进的算法而不是均匀采样。
2. baseli和论文改进的比较只有一个超参数：假定优先回放会倾向于高误差过渡，导致梯度空间也会非常大，所以论文把步长大小$\eta$减少到了`Double DQN`的4倍
3. 将baselines与上面提到的两种优先回放进行比较。
4. 对于超参数$\alpha$和$\beta_0$，做了一个比较粗糙地计算，得到两种情况下的最优值。
    - 基于排序的变量：$\alpha = 0.7, \beta_0 = 0.5$
    - 对于比例变量：$\alpha = 0.6, \beta_0 = 0.4$
5. 在所有游戏中使用一个超参数设置运行每个变量来生成结果，baseli也这么做。
    - 评估指标：最好策略的质量
6. 结果比较

||DQN||Double DQN（修正后）|||
|---|---|---|---|---|---|
| | baseline |rank-based |baseline| rank-based |proportional|
|Median|48\%|106\%|111\%|113\%|128\%|
|Mean|122\%|355\%|418\%|454\%|551\%|
|>baseline|-|41|-|38|42|
|>human|15|25|30|33|33|
|\#games|49|49|57|57|57|
可以发现效果有了很大改善。

## 讨论
对于**基于排序的优先性和比例优先性**，有下面两种可能。
1. 基于排序的优先性更好：
    - 优点：
        - 不会被异常影响，因而鲁棒性更强
        - 满足**重（zhong）尾**分布，保证了样本的多样性。
        - 从不同的误差中分层采样，使得在训练过程中保证了总的小批次的梯度在一个稳定的大小。
    - 缺点：rank使得算法忽视了相关错误的尺度，以致于会在利用错误分布时模型的表现不好。
2. 也可能二者差不多。那么此时可能是因为**在DQN算法中过多地使用了“剪枝”，从而去掉了异常**。

除此之外，论文还发现了一个奇怪的现象：**访问过的过渡的一些片段，在被从滑动窗口记忆丢弃之前，它们从不会被回放**，就算回放了，更多的还是第一次。

均匀采样暗含误差，因此优先回放解决了该问题，也会帮助解决第二大问题——时间上越近的过渡误差越大。这是由于旧的过渡有更多机会纠正自身错误，也是因为新的数据不太容易被值函数预测到。

## 拓展工作
1. 优先监督学习：从数据集非均匀采样，每个样本都应用基于上次“看到”的优先性
2. 离线策略回放：对于离线RL策略来说，有两种标准方法：剔除采样和重要性采样。
3. 探索反馈
4. 优先记忆
