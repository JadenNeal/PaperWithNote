---
title: 第二讲-监督学习和模仿学习（二）
date: 2019-12-30 14:52:34
tags: 强化学习
---
继续讨论的一个问题是，对于模仿学习来说，不需要增加很多数据也能表现很好吗？我们是否可以做到不让人坐下来看策略观察到的图像，机器也可以工作的很好呢？

<!--more-->

\\(DAgger\\)解决的是分布偏差的问题，但仔细想一想，如果模型很棒，没有误差（或误差非常小）呢？分布偏差的问题将不复存在。因此，找到这样一个模型也是解决问题的途径之一。从而我们可以改进模型，非常精确地去模仿动作，达到这一目的。但注意别过拟合。

## 为什么有可能拟合专家数据失败
原因有以下几点：

1. **非马尔可夫行为**。对于人类来说，即使一个人观察的图像包含了能够使他做出最优决策的所有信息，他还是有可能重复自己在过去决策中的选择。
2. **多模型行为**。即使人们表现出近似于马尔可夫的行为决策，他们也可能只是想要在相似的情况下做一些不同的事情。这么说有点难以理解。举个例子，一个人今天心情好，开车就会平稳一些，基本上会按照规则驾驶；如果一个人今天心情不好，那么开车就可能会激进一些，旋转方向盘的幅度会较大。又比如今天只用左手开车，或者只用右手...这些情况下展现的都是多模型行为。

## 非马尔可夫行为的讨论
如果怀疑示范者（即被模仿者）做的决策不仅取决于现在，并且还和他过去的经验有关，那么我们也可以让机器的决策变得也是既和现在也与过去有关。

前向NN就是仅依赖于当前观察数据的图像分类问题，就是说，如果机器看见一个事物两次， 那么两次都会做出同样的决策。但这显然不能满足人们的需要。

要想让机器和人类一样，能够使用过去的经验来进行决策，人们可以使用不同的NN的结构来做到这一点。最简单最大众的方法就是使用递归NN。

**如何做呢？**

首先明确一点，即使我们有所有的历史经验的数据，我们也不会完全去使用它。而是采用下图的方法。
<div align="center"><img src="https://s2.ax1x.com/2019/12/30/lMwCxx.jpg" alt="lMwCxx.jpg" border="0" ></div>
<center>图片来自cs294课程ppt</center>
对于输入图像的每一帧都采用卷积编码器来处理，并且所有的卷积编码器的权值将共享，最后，在所有的编码器上再放置一个RNN（循环神经网络），来提取卷积编码器塔的输出，并在时间上进行前向的积累。 

## 多模型行为的讨论
一个**引例**：  
对于一棵树，我们可以从左边绕过，也可以从右边绕过，两种方法都是可行的，但是综合这两种就不行了。  
上面的例子体现了一种多模型的方法。  

1. 如果我们进行的是一种**离散**的行为，多模型的方法完全没有问题。典型做法就是再网络结构的最后放置一个softmax函数。这可以呈现出基于随机变量类别的完整分布。因此，如果是离散决策行为的话，我们可以把行为集中再向左和向右的决策上，把中间的决策空出来，就可以了。
2. 如果进行的是**连续**的行为，情况会复杂一点。最普遍的做法是训练能输出连续行为决策的规则体系。  
  我们可以使用**高斯分布**实现它。  
  假设我们使用了**方均损失函数**，其实这就是在用高斯分布，原因在于方均误差就是高斯分布的对数概率。  
  **但是，倘若我们输出高斯分布的均值或者方差，结果都将是一个单峰的分布。用该模型拟合数据的话，就会得到绝对不想要的结果**。于是我们必须在高斯分布的基础上做一些调整，来使它能应对**多状态**的情况。于是有下面几种选择：  
    - 输出**混合的高斯分布**。这是最简单的一种方法。  
    - 输出**隐变量模型**。这个在数学上很优雅，但实现起来很困难。
    - **自动回归离散化(Autoregressive discretization)**。这种方法可能在形式上不太工整，但是相比较于隐变量模型，实现起来会容易不少。

## 混合高斯分布（mixture of Gaussians）
继续上面的讨论，混合高斯分布是为了应对多模型行为。

它的输出是N个均值N个方差，对于所有的选项都有一个标量权重，这些标量权重的和是1，即$$\sum 标量权重 = 1$$
它的模型如下：
<div align="center"><img src="https://s2.ax1x.com/2019/12/30/lMce2Q.jpg" alt="lMce2Q.jpg" border="0" /></div>
<center>注意这里是多峰的</center>
我们要在最后使用$softmax$函数，然后均值和方差就会在NN中起到一定作用，输出的就会使混合高斯分布模型的参数。

这种方法被称为**混合密度网络**。

在设计这种模型的时候，最关键的是**选择混合分布的元素数量**。通常来说我们不可能用有限数量元素的分布来表示任意的分布，因此，在现实中就意味着，在低维度的情况下，假设有一个或两个维度的行为决策，这种方法就会非常奏效。

然而在高维度的情况下，N个选择的机制就不太行得通。

## 隐变量模型（Latent variable models）
隐变量模型不要求我们对输出进行改动，输出仍然以一种像单峰高斯分布模型的简单形式存在。不同之处在于，该模型在NN的底部接入额外的输入，这些额外的输入是一些随机数，这些随机数可能是从高斯分布或者均匀分布中采样得到的。

这样我们就可以用这些随机性来训练我们的NN，以实质地改变输出的分布。

举例进行理解。如果我们面对向左向右的问题，网络将会在噪声进入的时候进行学习。对于一维的情况，若是负面（也可以说是负方向）的噪声，NN的输出就是向左，反之则是右。

**但是**，这种方法只是听起来不错。从原理上说，它是利用NN表现能力的技术（NN是一个普适的函数逼近器），所以NN可以把噪声转换为在输出时的任何分布。**但是**，在某种程度上NN需要噪声。在现实中怎么做呢？有些困难，简单地引入噪声然后用最大似然是行不通的（效果不理想），需要查找相关论文。

## 自动回归离散化
最简单的方法，但应用起来需要自己改动的地方较多。

1. 对于离散的动作决策，基本没有问题。
2. 但对于连续的动作决策，就没那么简单了。  
    - 如果能够将连续情况离散化地很好，那么使用该方法也没问题。
    - 但如果离散化的是一个高维的变量，那么问题就会变得很棘手，会产生维度灾难——维度指数性增长（二线性增长则是可以接受的）。**自动回归离散化**也正是主要应对这种情况提出来的。

它的思想如下：
1. 首先处理行为决策的第一个维度，对其进行离散化，使用softmax
2. 接着从分布中采样，就得到了行为决策的第一个结果——动作的维度
3. 将这个离散化采样送入别的NN，**让这个NN结合第一个维度的所有采样以及第二个维度的条件**，来预测一个离散化的结果。
4. 之后重复上述的过程即可。

## 模仿学习的问题
1. 人们需要提供数据，但这其实是很有限的。**深度学习在数据充足的情况下，学习效果是最好的**。这句话的隐藏含义是，有时候我们系统的性能瓶颈，在一开始就由数据量决定了。
2. 虽然人们在一些问题上擅长提供示范作用，但有些情况下却不能。比如人们擅长于走山路，驾驶等；但是对于一些更为精细的情况，比如告诉机器电机该转多快，演示如何打败世界冠军等，这些就做不来。因此，模仿学习并不能应用在所有领域。
3. 最后，人类可以自动学习，但是要想让机器自动学习，过程会很麻烦。后续会讲。
