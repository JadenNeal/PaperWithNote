---
title: 高级Q学习算法
author: Jaden
date: 2020年2月4日
tag: 强化学习
---
# 高级Q学习算法
## replay buffer
Replay Buffer是最基本的样本收集再采样的过程，是之前在做DDQN实验中使用的一种样本利用方式，原因是使用Q-learning算法进行在线方式学习时，会存在两个问题:

<!--more-->

1. 交互得到的序列存在一定相关性（因为在线学习往往就是得到一个就立马拿来训练 ）。而机器学习模型对训练样本的假设是独立、同分布的，所以序列打破了这种独立同分布特性，因此效果不太好。
2. 交互样本使用的效率过低。因为每次要使用一定的时间获取一个batch的样本才能完成一次训练，所以对样本的获取是有些慢的，而且在线学习方式往往会将学习后的样本直接丢弃，这样下来利用的效率不高。

采用了**Replay Buffer**的Q-learning算法如下：
1. 使用策略收集数据，放入缓存$B$中
2. 从缓存$B$中取出一个批次
3. 计算梯度。
4. 不断重复上述两步即可。

## Q-learning与回归的关系
从上面的使用replay buffer的Q-learning而言，2、3两步更像是在做回归运算，虽然本质不是回归。第三步做了两件事，一个是梯度下降，二是移动目标（*target*），因此也称为**target networks**。

## Q-learning with target networks
Q-learning with replay buffer and target networks:
1. 保存target networks的参数：$\phi^\prime \leftarrow \phi$
2. 使用策略收集数据，放入缓存$B$中
3. 从缓存$B$中取出一个批次
4. 计算梯度

在第四步，目标在内部循环中并不改变；第2、3、4步可以看成是**监督学习**

## DQN
由replay buffer和target network组成经典的深度Q-learning网络。
算法如下：
1. 采取动作$a_i$，使之满足$(s_i, a_i, s_i^\prime, r_i)$，然后将其加入到缓存$\beta$中。
2. 从缓存$\beta$中均匀采样，得到小批次（mini-batch）$(s_j, a_j, s_j^\prime, r_j)$
3. 使用target network计算$y_j = r_j + \gamma\max_{a_j^\prime}Q_{\phi^\prime}(s_j^\prime, a_j^\prime)$
4. 更新策略$\phi$
5. 更新$\phi^\prime$：每$N$步复制一次$\phi$

## Fitted Q-iteration and Q-learning
首先定义三个过程：
1. 过程一：收集数据（中途可能丢弃旧的数据）
2. 过程二：更新目标
3. 过程三：Q函数回归

下面进行比较：
1. 在线Q-学习：立即抛弃旧数据，三个过程以同一速度运行
2. DQN：1、3两个过程同速运行，过程2运行较慢
3. Q-iteration：这是一系列循环，3在2中循环，而2又在1中循环

## 如何优化目标
附加一个近似项。后面就是一堆公式推导了。优化$\theta$。

## 实用方法
1. 自然梯度下降
2. TRPO（Trust Region Policy Optimization）
3. 直接使用重要性（Important Sampling, IS）采样
