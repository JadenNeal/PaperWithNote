# 价值函数介绍

# 价值函数介绍
本次内容：
- 仅使用critic，不用actor
- 从价值函数中提取策略
- Q-learning算法
- 拓展：连续动作，提升
  
目标：
- 理解价值函数对于策略的提升
- 理解Q-learning算法
- 理解Q-learning实际应用需要考虑的因素

## 能否完全忽略梯度下降
能否只从*critic*中提取policy，而不使用前面的策略呢？答案是**肯定**的。
首先回顾下面一些定义：
### 优势函数
$A^\pi(s_t, a_t)$：根据策略$\pi$得出$a_t$比平均动作值好多少。倘若每次都取最大值（即最好的动作），那么每次实际采取的动作**至少**与采取$\pi$中的动作$a_t$一样好，并且这个对于每个单步时间都是正确的。

### 策略迭代
**进阶的想法**：策略迭代。
策略迭代算法：
1. 估计$A^\pi(s_t, a_t)$
2. 将$\pi^{\prime} \rightarrow \pi$

第二步非常容易，所以主要是解决第一步。注意这里有$$A^pi(s, a) = r(s, a) + \gamma E[V^\pi(s^\prime)] - V^\pi(s)$$因此估计$A^\pi$就变成了估计$V^\pi$

估计$V^\pi$是个**动态规划问题**，为了简化分析，给定一下假设（不可实际应用）：
- 已知转移概率$p(s^{\prime}|s, a)$
- $s$和$a$都是离散的，并且足够小，计算机可以存储。

使用表格法。

## max是个坏方法
在值函数迭代算法中，每次都要取最大值。这意味着需要在不同动作时找到表达式的不同值。比如有5哥不同的动作，那么需要各自尝试来估计5哥不同的数字，但除了第一个状态，再也回不到其他的状态，并且尝试两种不同的动作。因为一般情况下状态空间非常大，再次回到同一状态非常困难。因此说**max是一种坏方法**。

解决方法就是不更新价值函数$V^\pi$，而是直接更新$Q^\pi$

## Fitted Q-iteration
完整算法如下：
1. 使用policy收集数据
2. 更新$y_i$
3. 更新$\phi$

## 有表格价值函数学习理论
涉及到过多的数学推导，这里省略。

## 无表格价值函数学习
价值函数迭代算法（使用贝尔曼算子$\beta$）
$$V \leftarrow \beta V$$

最后补充两个术语：
1. $V^\pi$：策略$\pi$的价值函数，是$critic$所做的事情
2. $V^*$：可选策略$\pi^*$的价值函数，这是价值函数迭代的结果。
