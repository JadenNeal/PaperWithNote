# ACKTR

本论文思维导图如下：
<center><img src="https://s1.ax1x.com/2020/03/27/GiWIEV.png" alt="GiWIEV.png" border="0" /></center>

<!--more-->

## 摘要
论文的主要工作是**拓展自然梯度，并使用带有信任域的K-FAC算法优化Actor和Critic**，因此取名为`ACKTR`（音同Actor）。它有如下特点：
1. AC（`Actor-Critic`）框架下，第一个可伸缩的信任域的，自然梯度方法。
2. 直接从原始像素输入学习的方法，可在非平凡的任务中应用离散或连续控制。

测试：
- 离散任务：`Atari`游戏
- 连续任务：`MuJoCo`环境

结果：对比当时最好的算法，样本效率提升了2-3倍，得到的奖励更高。

## 介绍
### DRL和分布式方法
**DRL**成果不少，但是采用的是`SGD`及其相关的**一阶**算法，这会导致搜索权重空间效率不高，花费时间巨大；后来有人提出了**分布式方法**，让多个agent同时与环境进行交互，减少了计算时间，但是样本效率又降低了。

### 样本效率和自然梯度
**样本效率**是RL关心的主要问题之一。**自然策略梯度使用自然梯度下降来更新梯度**，从而可以有效减少样本大小。注意，自然梯度下降遵循`Fisher`指标，会沿着最快的下降方向。

### TRPO的不足
**自然梯度虽好，但是需要对Fisher矩阵求逆，所以精确计算是不可能的**。`TRPO`则采用`Fisher`向量乘积的方法避免了直接求逆。但是，`TRPO`参数更新过于复杂，因此不实用，并且`TRPO`的样本效率也较低。

### K-FAC算法
`K-FAC`，K系数曲率逼近，是一个可伸缩的对自然梯度的逼近。`K-FAC`每次更新的成本与`SGD`相当。
`K-FAC`的特点：**保持曲率信息的平均值，允许使用小批量**。
该特点可以用来提升样本效率。

### ACKTR简介
特点：
1. 使用`K-FAC`逼近自然策略梯度，快速对协方差矩阵求逆
2. 用高斯牛顿法优化值函数，从而拓展了自然梯度算法。
3. 计算成本只比基于`SGD`的算法高10-25%，但是样本效率提升了2-3倍，奖励回报也大大提高。

## 方法
###  AC中的自然梯度
学习`Critic`可以看作是一个最小二乘函数逼近问题，选择的**二阶**算法试最为常用的高斯-牛顿法。
对于`Critic`，也就是`Value Function`，一般来说会输出一个值，但这里**论文假设其输出的是一个高斯分布**，并在实践中确定$\sigma = 1$效果最好。

### 步长选择和信任域优化
自然梯度的传统做法都是用`SGD`来更新，但这样会导致过大的更新，可能会导致算法过早地收敛到一个接近确定性的策略。
因此提出了**信任域**方法。根据这种方法，可以缩小更新范围，以最大某个规定的步长修改策略分布。

## 实验
### 实验目的
做这些实验主要是为了研究以下问题：
1. ACKTR与现有方法及常见二阶算法在**样本效率和计算成本**上的比较；
2. 哪项参数可以让`Critic`的优化更好；
3. 对比一阶算法，带有`batch size`伸缩的`ACKTR`的表现如何。

### 实验环境
1. 离散控制：`Atari`游戏，平台是`OpenAI Gym`，由`Arcade Learning Environment`驱动；
2. 连续控制：`MuJoCo`环境，平台还是`OpenAI Gym`。

### baseline
- A3C
- A2C
- TRPO

### 实验步骤
1. 使用`AC`框架，
    - 对`Actor`：也就是`Policy function`，使用自然梯度下降；
    - 对`Critic`：也就是`Value function`，使用高斯牛顿法。

2. 然后用`K-FAC`分别对A、C进行估计；
3. **离散部分**，在`Atari`的6个游戏中，运行1千万个时间步（**注：1个时间步为4帧**），比较`ACKTR`、`A2C`和`TRPO`的奖励；
4. 运行5千万个时间步，比较`ACKTR`、`A2C`和`TRPO`的最后100个回合的平均奖励；
5. **连续部分**，在`MuJoCo`上的8个任务上，运行1百万个时间步，比较三者的最终奖励；
6. 运行3千万个时间步，比较`ACKTR`、`A2C`和`TRPO`的最高的10个回合的平均奖励，以及达到规定标准（由一篇文献的结论）需要的回合数。

### 更多技术细节讨论
1. `K-FAC`采用的是一种分布式的实现——异步计算矩阵的逆。
2. 使用信任域进行重（chong）伸缩更新，类似于`TRPO`的做法。
3. 最为关键的一点，使用**移动平均**把Fisher矩阵的数据进行累计，从而可以得到更好的估计。

### 实验结果
#### 离散控制
在`Atari`的6个游戏中，运行1千万个时间步（**注：1个时间步为4帧**），比较`ACKTR`、`A2C`和`TRPO`的奖励:
<center><img src="https://s1.ax1x.com/2020/03/27/Gi1YAU.png" alt="图1" border="0"></center>

可以看到在这6个游戏中，`ACKTR`的表现碾压其他两个。
接着训练5千万次，比较最后100个回合的平均奖励，以及要达到人类水平所需要的回合数。
<center><img src="https://s1.ax1x.com/2020/03/27/Gi1lXq.png" alt="表1" border="0"></center>

可见，在`Beamrider`、`Breakout`、`Pong`和`Q-bert`游戏中，`ACKTR`所花费的回合数都比`A2C`至少少了3倍。此外，在`Space Invaders`游戏中，`A2C`失败了，而`ACKTR`的分数大概是人类水平的12倍。不仅如此，`ACKTR`在各个游戏的分数也比`A2C`的高不少，`ACKTR`的优势可见一斑。
#### 连续控制
在`MuJoCo`上的8个任务上，运行1百万个时间步，比较三者的最终奖励：
<center><img src="https://s1.ax1x.com/2020/03/27/GiYCLR.png" alt="ACKTR3" border="0"></center>

`ACKTR`的优势很明显。
运行3千万个时间步，比较`ACKTR`、`A2C`和`TRPO`的最高的10个回合的平均奖励，以及达到规定标准（由一篇文献的结论）需要的回合数：
<center><img src="https://s1.ax1x.com/2020/03/27/GiYpQJ.png" alt="ACKTR4" border="0"></center>

### Critic优化的更好的范数
前人工作都是只优化`Actor`，`ACKTR`将`Critic`也纳入优化范围中。不同的是范数采用的是论文自己定义的：
范数$||\cdot||_B$来自于$||x||_B = (x^TBx)^{\frac{1}{2}}$，其中$B$是一个半正定矩阵。
结果发现，**无论用哪种范数来优化Critic，ACKTR的表现总是或多或少地比A2C好，尤其是样本效率和回合分数方面**。

### 不同的batch size
设置了两种`batch size`：160和640。
- `ACKTR`：表现不变。
- `A2C`：`batch size`越大，效果越差。

这说明在较大`batch size`下，使用`ACKTR`效果更好。

## 个人思考
`ACKTR`的确是一个好算法，在各个方面的表现都超过了`A2C`和`TRPO`算法，尤其是样本效率以及回合奖励方面。

本篇论文值得借鉴的地方有两点：
1. 创新点：融合各种算法，`ACKTR`的实质就是将`K-FAC`应用于AC框架，仅仅是实践上的创新，使得模型有了更好的表现。
2. 方法论：主要就是对比试验，与各种算法进行比较，固定训练时间比结果，固定结果比耗费时间，等等，这就从许多角度论证了该算法的优越性。

但是，不难发现，`ACKTR`也有缺点，**就是样本效率的问题**。在最后的讨论中，作者用了160和640两种`batch size`进行试验，结果就是`ACKTR`在这两种情况下的表现并无差别，而`A2C`的表现却随着`batch size`的提高而下降了，这是在比烂，因为`ACKTR`的表现并没有更好。当然这可能属于`batch size`对模型的影响，表现不变差就可以称得上“较好”了。
最后，作者也给出优化思路，就是针对样本效率问题进行改进，参见`BDPI`算法；未来的工作方向则是考虑将不同的算法融合起来，应用起来，就有可能得到表现更好的模型。
