# 经典DQN

## 摘要
论文提出了深度学习模型成功应用于强化学习，可以说是将二者结合起来。模型为卷积神经网络，算法是`Q-learing`，输入为原始像素，输出是价值函数，可以估计未来的奖励。论文在7个Atari实验中进行测试，有6个游戏都超过了传统做法，其中有3个还超过了人类。

<!--more-->

## 简介
RL的一个长久的挑战就是直接从高维感官输入（比如视觉和语音）中学习控制智能体。很多成功的RL应用都是依赖于人工进行特征标注，因此这样的系统也严重依赖于特征表示的质量。

DL的进步使得从原始感官数据中提取高维特征称为可能，从而在CV和NLP领域取得重大突破。这些方法**使用一系列的NN，包括CNN，多感知机，受限的玻尔兹曼机和RNN，并且利用了监督、无监督学习**，论文受此启发，将这种技术应用在含有感官数据的RL工作中。

### RL的困难
一直以来，RL都对DL提出了几大挑战：
1. DL模型都需要巨量人工标注的训练数据，**而RL则相反**，RL必须能够从频繁稀疏、有噪声和延迟的标量奖励中学习。与监督学习中的输入输出的直接联系相比，动作与后来的奖励之间的延迟将**看起来非常尖锐**。
2. 大多数DL算法都假设数据是独立的，**然而**RL中的的状态序列都是高度相关的。
3. DL中的数据分布是不变的，**然而**RL中的数据分布会随着智能体学习到新的行为而改变。

### 论文的工作
论证了CNN在RL上的可行性，能够克服上述的一系列挑战。
大概做法：用`Q-learning`算法变量来训练网络，用`SGD`来更新参数，为了减少数据间的相关性和非稳态分布，还使用了**经验回放**。

最后，在Atari游戏上应用该模型，游戏环境的高维输入是`210 x 160 RGB, 60Hz刷新率`，模型只从视频输入中进行学习，7个游戏中由6个都超过了传统做法，效果很好。

## 背景
考虑到任务中智能体与环境$\varepsilon$（即Atari模拟器）进行交互，得到一系列的状态，观察和奖励。动作输入到模拟器中，修改模拟器的内部状态和游戏分数。由于环境$\varepsilon$是随机的，因此智能体并不能观测到模拟器的内部状态，只能从模拟器中观察到图像$x_t \in \mathbb{R}^d$，这是原始像素值得一个向量，代表当前的图像。此外智能体接受的奖励$r_t$代表着游戏分数的改变。需要注意游戏分数大体上取决于整体的动作和观察的优先序列，关于动作的反馈也只是在数千个时间步之后。

由于智能体仅仅观察当前窗口的图像，因此该任务是部分可观察的，许多模拟器状态都会被误认为其他状态，比如他就不可能仅从当前窗口理解全部的状态$x_t$。因此论文考虑了一系列的动作和观察，$s_t = x_1, a_1, x_2, a_2, \ldots,a_{t-1}, x_t$，并基于这些序列学习游戏策略。模拟器中的所有序列都假定在有限的时间步内终止，这种形式产生了一个大而有限的马尔可夫决策过程（MDP），其中每个序列都是一个不同的状态。因此，仅仅通过使用$t$时刻的状态$s_t$，就可以将标准的RL应用于MDP上。

智能体的目标是通过选择动作与模拟器进行交互，来最大化未来的奖励。论文假设未来奖励在每一步有一个**折扣因子**$\gamma$，定义了未来的衰减回报为
$$R_t = \sum_{t^\prime = t}^T\gamma^{t\prime - t}r_{t\prime}$$这里$T$是时间步结束时间。
又定义了优化的动作值函数$Q^\star(s, a)$，表示遵循策略得到的最大期望回报。
$$Q^\star(s, a) = \max_\pi E[R_t|s_t = s, a_t = a, \pi]$$这里$\pi$是一个序列到动作的映射（也可以说是基于动作的分布）。

$Q^\star(s, a)$遵守一个重要定理——**贝尔曼等式**。这么说基于以下**理由**：倘若序列$s\prime$的优化值函数$Q^\star(s\prime, a\prime)$对于所有的动作$a\prime$都是已知的，那么优化策略就会选择$a\prime$来最大化$r + \gamma Q^\star(s\prime, a\prime)$的期望值，即
$$Q^\star(s, a) = E_{s\prime\sim\varepsilon}\left[r + \gamma\max_{a\prime}Q^\star(s\prime, a\prime)\vert s, a\right]$$ 许多RL算法背后的基本思想是**使用贝尔曼等式作为迭代更新来预测动作-价值函数**，即
$$Q_{i+1}(s, a) = E[r + \gamma\max_{a\prime}Q_i(s\prime, a\prime)\vert s, a]$$  这样的价值迭代算法会收敛于价值函数，用数学语言则表示为$Q_i \rightarrow Q^\star , i\rightarrow \infty$
但是，**这种基本方法在实际中完全不实用**，理由是由于每个序列的动作-价值函数都是分别预测的，没有任何的一般性归纳。然而，**使用函数逼近器去预测动作-价值函数却很常见**，即$Q(s, a;\theta) \approx Q^\star(s, a)$。在RL中这是一个典型的线性函数逼近器，当然有时也有非线性的，比如神经网络，人们通常将权重为$\theta$的神经网络函数逼近器称为Q网络，用每次迭代$i$后最小化损失函数$L_i(\theta_i)$来训练Q网络：
$$L_i(\theta_i) = E_{s, a\sim\rho(\cdot)}\left[(y_i - Q(s, a;\theta_i))^2\right]$$ 这里$y_i = E_{s\prime\sim\varepsilon}[r + \gamma\max_{a\prime}Q(s\prime, a\prime;\theta_{i-1})\vert s, a]$是每次迭代的目标，$\rho(s, a)$是一种基于序列$s$和动作$a$的概率分布，论文称之为**动作分布**。优化损失函数$L_i(\theta_i)$时，前一次迭代$\theta_{i-1}$的参数保持不变，值得注意的是基于网络权重的目标，与监督学习中的目标相反，后者在学习开始前是固定的。将损失函数与权值进行微分，得到如下梯度：
$$\nabla_{\theta_i}L_i(\theta_i) = E_{s, a\sim\rho(\cdot);s\prime}\sim\varepsilon\left[\left(r + \gamma\max_{a\prime}Q(s\prime, a\prime;\theta_{i-1}) - Q(s, a; \theta_i)\right)\nabla_{\theta_i}Q(s, a; \theta_i)\right]$$  相比于计算上述梯度中的全部期望值，论文采取了通过`SGD`来优化损失函数。如果每次时间步后权重都更新了，那么期望就会被来自于行为分布$\rho$和模拟器$\varepsilon$的单次样本值分别代替，于是就得到了类似于`Q-learning`的算法。

值得注意的是，这种算法有两个特点：
1. **无模型**的，他直接用来自模拟器$\varepsilon$的样本解决了RL任务，而不需要明确地构造$\varepsilon$的估计。 
2. **离线策略**：该算法从贪心策略（$a = \max_aQ(s, a; \theta)$）学习，服从行为分布，又能确保对状态空间有足够的探索。在实际应用里，行为分布常常是由$\epsilon$贪心策略选择，服从参数为$1-\epsilon$的策略，并且以概率$\epsilon$选择动作。

## 之前的工作
TD-gammon，RL在博弈应用的经典，使用类似于`Q-learning`的五模型算法，然后用多层神经网络作为函数逼近。**然而**，后续对TD-gammon的深入则不那么成功，这一度让人们相信`TD-gammon`只是对*西洋双陆棋*有效果的一个特例。

而且，人们还发现，将无模型的RL算法例如`Q-learning`与非线性函数逼近器融合到一起（或者是离线策略学习），会导致Q网络发散。于是**后续的RL的主要研究工作就是保证更好的收敛**。

DNN用于估计环境$\varepsilon$，受限的玻尔兹曼机则用于估计价值函数，或者策略。此外，**梯度时差方法部分地解决了发散问题**。但是，这些方法还没能拓展到非线性控制部分。

之前的研究与DQN最相似的则是`NFQ`（神经拟合Q学习）。NFQ优化了损失函数序列，使用`RPROP`（弹性反向传播）算法更新Q网络的参数。
NFQ与DQN的对比
1. 首先是参数更新方式。
   - NFQ使用的是一个批量更新，每次迭代的计算成本与数据集的大小成正比;
   - 而DQN考虑的是随机梯度更新，每次迭代的恒定成本低，并且可以扩展到大型数据集。
2. 然后是学习方式。
    - NFQ使用深度自编码器学习低维的任务表示；
    - DQN则直接从视觉输入，端到端地应用强化学习。因此可以学习与辨别动作值直接相关的特征。

## DQN概述
与`TD-gammon`和与之类似的在线算法不同，DQN利用的是**经验回放**。分为以下几步：
1. 在数据集D中存储这些经验，$e_t = (s_t, a_t, r_t, s_{t+1})$，把大量的回合（episode）池化进回放记忆。
2. 在算法的内循环中，对从存储样本池中随机抽取的经验样本应用Q-学习更新，或小批量更新。
3. 做完经验回放后，智能体根据$\epsilon-$贪心策略挑选一个动作并执行。
4. 由于使用随机长度的记忆作为NN的输入很困难，因此DQN使用固定长度的记忆表示。

### DQN算法
完整算法如下：
<div align=center><img src="https://s1.ax1x.com/2020/03/22/85be6s.png" alt="85be6s.png" border="0" />
使用经验回放的深度Q学习</div>

### DQN的优点
1. 经验的每一步都有可能用于大量的权值更新，所以样本效率更好；
2. 前面提到过多次，直接从连续样本值学习效率低下，这是由于样本间的极强的相关性，随机样本可以打破这种相关性，从而降低了更新的方差；
3. 当学习策略时，当前参数将确定参数所使用的下一个数据样本。

## 实验
1. 正奖励均为1；负奖励均为-1；未变状态均为0
2. 使用`RMSProp`算法，最小批次大小是32，训练策略为$\epsilon-$贪心策略，前100万帧参数$\epsilon$从1到0.1线性下降，随后固定在0.1。论文中一共训练了1000万帧，并使用了100万个最近帧的重放记忆。

### 稳定性
虽然缺少理论上的收敛保证，但经过实践，DQN可以使用RL算法和SGD训练大的NN。

### 性能比较
见下图。
<div align=center><img src="https://s1.ax1x.com/2020/03/22/8oPOjx.png" alt="8oPOjx.png" border="0"></div>

上面五行比较的是DQN和其他算法的性能，均采用了$\epsilon-$贪心策略，参数$\epsilon = 0.05$
下面三行比较的是`HNeat`和DQN**单回合更新**的最佳表现结果，前者采用确定性策略，每次得到的分数相同；而DQN则采用$\epsilon-$贪心策略，$\epsilon = 0.05$。
可以发现，DQN的表现效果极佳。
