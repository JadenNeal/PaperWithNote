---
title: '第二讲-监督学习和模仿学习（一）'
date: 2019-12-29 14:32:52
tags: 强化学习
---
[cs294-B站](https://www.bilibili.com/video/av39816961?t=1435)，还有一个cs285，是cs294的新版，有新内容，但是还没有完整的中文翻译版，所以没看。二者区别不大。

<!--more-->

## 监督学习
本课程用到的符号是soa，即state, observation, action。在控制论中，X表示state，U表示action，两种表示的意义是一样的。

## 模仿学习
模仿学习是一种最简单的学习策略，完全照搬别人的经验、按照别人的描述进行学习。但是，这些行为中可能有坏的行为也会被机器学习到，因此，这种方法效率不高。

有几点原因：

1. 在现实中可能出现训练数据中没有见过的物体，比如训练是一辆白色大卡车，而现实中遇到的却是黑色的汽车，从而导致机器失去反应能力。
2. 人也会犯错。也就是说，机器可能学习到错误的、坏的行为，而这些错误行为来源于机器模仿的对象。
3. 在训练数据中，可能会有不同行为不同动作。对于人类来说，他做的决定基于他的过去看到的东西。而策略只能根据当前看到的来决策。
4. 上述三点都是学生的猜想，老师给出的解答是：**误差问题。**在训练过程中，预测与现实总会有或多或少的误差，误差在一定范围内就可以认为预测结果可以接受。但现实情况是，模仿学习的误差会累积，从而造成预测结果与现实产生非常大的差距，最后会出现训练数据中完全没有过的情况。

解决方法也很“简单”：

- 增大数据集，尽可能地在训练数据中涵盖所有可能的情况。从而可以使机器学习到多种应对策略。
- 由于不可能涵盖所有的样本，所以增大数据集的方法不太适用。因此，可以人为地引入噪音，使得机器能够学习到纠正行为的方法。
- 另一种方法，不改变预测的策略，而是让数据集更“聪明”。这种方法称为\\(DAgger: Dataset \space Aggregation\\)

### DAgger
**让数据集更聪明的算法。**

符号说明： 

-  \\(P_{data}(O_t)\\)：人类在现实中观察得到的数据集。机器就是从该数据集中采样进行学习。  
-  \\(\pi_{\theta}(a_t|o_t)\\)：一种策略。在实际中观察到的是一种完全不同的分布，\\(\\theta\\)的变化将引入误差。在这种策略下观察得到的数据集为\\(P_{\pi \theta}(O_t)\\)。

从而前面所讨论的模仿学习有时候效果不好的原因就可以表述为两个数据集并不一致。  
下面是\\(DAgger\\)算法步骤：

1. 类似于行为克隆。不去收集\\(P_{data}(O_t)\\)的数据，而是\\(P_{\pi \theta}(O_t)\\)，这样一来它们就相等了。就是说，收集的是\\(P_{\pi \theta}(O_t)\\)，训练的也是\\(P_{\pi \theta}(O_t)\\)，那么二者就一致了（可以把收集的看成\\(P_{data}(O_t)\\)）。  
怎么做呢？ 

    - 运行\\(P_{\pi \theta}(O_t)\\)
    - 给\\(a_t\\)打上标签

2. 换句话说，**运行过程是：**

    - Collection: 机器收集专家数据集\\(D\\)
    - Run: 运行\\(\pi_{\theta}\\)策略，记录观察结果，得到新的数据集\\(D_{\pi}\\)
    - Observation: 将结果交给“专家”观察
    - Label: 专家为这些动作数据\\(a_t\\)打上标签
    - Aggregate: 机器将\\(D\\)和\\(D_{\pi}\\)混合，得到新的\\(D\\)

这过程很烦，需要足够长的时间，但确实能解决分布不一致的问题。

### DAgger的问题
问题出在最关键的第三步——**要求人来打标签**。这一步具有很大的不确定性，非常依赖于算法所应用的场景。如果是在围棋中，那么人们可以为机器的选择的动作打上标签；但是在开车等类似的任务中，人们很难通过该动作判断它对全局的优劣性。因此，这对人来说是一个非常不自然的状态。并且，在实践中，这一过程（指第三步）是非常耗费资源的。
