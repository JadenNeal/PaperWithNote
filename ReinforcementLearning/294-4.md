---
title: 策略梯度简介（一）
date: 2020-01-08 22:24:02
tags: 强化学习
---
讲解policy Gradients的概述及问题。

<!--more-->

## 目标-最大化奖励
在强化学习中，我们需要找到一种策略，为之加入某些状态，最后得到一个奖励最大的策略。有下面这个函数。
$$\theta^* = \arg \max_{\theta} E_{\tau\sim p_{\theta}(\tau)}\left[\sum_tr(s_t, a_t)\right]$$
这是一个对复杂概率分布求期望的形式，并且数据维度很大（为$R^n$）。  
对它求解是非常困难的，只能求取近似值。

现在，记$$J(\theta) = E_{\tau\sim p_{\theta}(\tau)}\left[\sum_tr(s_t, a_t)\right]$$
那么该如何估计呢？

答案是对分布进行采样，更进一步说是**逐步地执行策略**$\pi_\theta$。因此，得到的估计式为
$$J(\theta) = E_{\tau\sim \pi_{\theta}(\tau)}\left[\sum_tr(s_t, a_t)\right] = \frac{1}{N}\sum_i\sum_tr(s_{i,t}, a_{i,t})$$

## 提升估计值
得到了估计值后，我们想要提升该估计值，标准的做法是求取$J(\theta)$对变量$\theta$的梯度，然后沿着**梯度方向进行操作**。这里我们的目标是使$J(\theta)$最大化，因此我们需要对$J(\theta)$进行**梯度上升**。为此我们需要先求得梯度，再对其乘以一个系数，最后加到网络参数上。

### 求梯度
首先，将奖励函数的求和记为以下式子：$$r(\tau) = \sum_{t=1}^Tr(s_t, a_t)$$
显然，$r(\tau)$表示奖励函数的叠加。

由此，我们可以将$J(\theta)$改写为积分式：$$J(\theta) = \int\pi_{\theta}(\tau)r(\tau)\,d\tau$$

所以，$J(\theta)$的梯度就是$$\nabla_{\theta} J(\theta) = \int\nabla_{\theta}\pi_{\theta}(\tau)r(\tau)\,d\tau$$
其中，$\nabla_{\theta}$表示变量是$\theta$的梯度。  

在数学教材中，有下面的公式：
$$\pi_{\theta}(\tau)\nabla_{\theta}\log \pi_{\theta}(\tau) = \pi_{\theta}(\tau) \frac{\nabla_{\theta}\pi_{\theta}(\tau)}{\pi_{\theta}(\tau)} = \nabla_{\theta}\pi_{\theta}(\tau)$$

于是，就有$$\nabla_{\theta} J(\theta) = \int\nabla_{\theta}\pi_{\theta}(\tau)r(\tau)\,d\tau $$$$= \int\pi_{\theta}(\tau)\nabla_{\theta}\log \pi_{\theta}(\tau)r(\tau)\,d\tau = E_{\tau\sim \pi_{\theta}(\tau)}[\nabla_{\theta}\log \pi_{\theta}(\tau)r(\tau)]$$
而$\pi_{\theta}(\tau)$是一系列由状态和动作组成的**联合概率**的缩写，即
$$\pi_{\theta}(\tau) = \pi_{\theta}(s_1,a_1,\dots,s_T,a_T) = p(s_1)\prod_{t=1}^{T}\pi_{\theta}(a_t|s_t)p(s_{t+1}|s_t, a_t)$$
两边取对数，得：$$\log\pi_{\theta}(\tau) = \log p(s_1)+ \sum_{t=1}^{T}\log\pi_{\theta}(a_t|s_t) + \log p(s_{t+1}|s_t, a_t)$$
将此式代入到$\nabla_{\theta} J(\theta)$中，则第一项和第三项对$\theta$求梯度均为0，所以，$$\nabla_{\theta} J(\theta) = E_{\tau\sim \pi_{\theta}(\tau)}\left[\left(\sum_{t=1}^T\nabla_{\theta}\log \pi_{\theta}(a_t|s_t)\right)\left(\sum_{t=1}^Tr(s_{i, t},a_{i,t}\right)\right]$$
和之前一样，这个式子，同样求不出解析解，因此必须像前面那样用采样值来估计它得近似值。则有$$\nabla_{\theta} J(\theta) \approx \frac{1}{N}\sum_{i=1}^N\left[\left(\sum_{t=1}^T\nabla_{\theta}\log \pi_{\theta}(a_t|s_t)\right)\left(\sum_{t=1}^Tr(s_{i, t},a_{i,t}\right)\right]$$
最后一步，将更新的策略赋值回去即可改进策略。$$\theta \leftarrow \theta + \alpha \nabla_{\theta}J(\theta)$$

不过，值得一提的是，该流程实现的效果不好，后续会有优化步骤。

## 策略梯度的问题
**策略梯度的问题实际上是方差问题**。通过有限的样本去估计梯度，需要重复多次，每次计算都得到不同的估值，对于少量有限的样本来说，这些估值会有很大的差异。

究其原因，是因为该方法是沿着梯度方向而不是直线轨迹方向进行优化，因此估值会“到处乱跑”。如果摆动的幅度大于我们朝着目标点所作的优化，那么最后会收敛在一个较差的位置，或者需要相当长的时间才能收敛，或者需要非常小的步进。

## 降低方差
### 引入因果性假设
首先引入一个因果性的假设：当$t < t^{'}$时，策略在$t^{'}$时刻的奖励不会影响$t$时刻的。通俗点说就是未来不会影响过去。

从而可以得到$$\nabla_{\theta} J(\theta) \approx \frac{1}{N}\sum_{i=1}^N\left[\left(\sum_{t=1}^T\nabla_{\theta}\log \pi_{\theta}(a_t|s_t)\right)\left(\sum_{t^{'}=t}^Tr(s_{i, t^{'}},a_{i,t^{'}}\right)\right]$$

解释一下为什么这么做就可以减小方差。注意到上面的式子$t^{'}$是从$t$开始的，已经减去了前面的结果（即$1\sim t$得部分），这个和变小了，所以方差也变小了。

### 引入基线（Baseline）
需要明确的一点是，人们对不直观的梯度下降问题的策略是让策略梯度拿到好动作，使其可能性增加，而坏的动作的可能性则降低。

但事实不完全是这样，假设奖励都是正的，那么所有动作的概率都会提高。

**我们真正想做的，就是提高好的动作的可能性，降低坏的动作的可能性**。  

做法就是用每一次奖励减去奖励的均值，然后进行比较，正的则增加可能性，负的则减少可能性。

首先，记$$b = \frac{1}{N}\sum_{i=1}^Nr(\tau)$$
表示奖励的均值，称之为**基线Baseline**。

因此，可以将上面求梯度的式子改写为
$$\nabla_{\theta} J(\theta) \approx \frac{1}{N}\sum_{i=1}^N\nabla_{\theta}\log\pi_{\theta}(\tau)[r(\tau) - b]$$

为了证明这个式子的正确性，只需要证明
$$E\left{\frac{1}{N}\sum_{i=1}^N\nabla_{\theta}\log\pi_{\theta}(\tau)[r(\tau) - b]\right}$$

$$= E\left{\frac{1}{N}\sum_{i=1}^N\left[\left(\sum_{t=1}^T\nabla_{\theta}\log \pi_{\theta}(a_t|s_t)\right)\left(\sum_{t^{'}=t}^Tr(s_{i, t^{'}},a_{i,t^{'}}\right)\right]\right}$$

其中$E[\cdot]$表示期望。

两式相减，化简即可得到最终需要证明的是
$$E[\nabla_{\theta}\log\pi_{\theta}(\tau)b] = 0$$

这其实也很容易，利用数学教材中的公式$$\pi_{\theta}(\tau)\nabla_{\theta}\log \pi_{\theta}(\tau) = \pi_{\theta}(\tau) \frac{\nabla_{\theta}\pi_{\theta}(\tau)}{\pi_{\theta}(\tau)} = \nabla_{\theta}\pi_{\theta}(\tau)$$

于是$$E[\nabla_{\theta}\log\pi_{\theta}(\tau)b] = \int\pi_{\theta}(\tau)\nabla_{\theta}\log\pi_{\theta}(\tau)b\,d\tau $$$$=\int\nabla_{\theta}\pi_{\theta}(\tau)b\,d\tau = b\nabla_{\theta}\int\pi_{\theta}(\tau)\,d\tau = b\nabla_{\theta}1 = 0$$

得证。

这就意味着从**有限样本**的奖励中减去或者增加一项都不会改变期望，而方差则会改变，我们想要的结果就是**期望不变，方差减小**。

### 最佳基准线
值得一提的是，上面的$b$不是唯一的（但均值基线相当有效），可以任意取，其中能够使得方差最小的$b$便称之为**最优Baseline**

想来也很简单，最优的则是方差梯度为0的点。不再赘述过程，直接贴出结果。
$$b = \frac{E[g(\tau)^2r(\tau)]}{E[g(\tau)^2]}$$
