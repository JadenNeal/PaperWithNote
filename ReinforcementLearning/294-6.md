# Actor-critic算法简介

关于`actor-critic`的一个简介。

## State & state-action value functions

首先引入以下变量：
$Q^{\pi}(s_t, a_t) = \sum_{t^\prime =t}^TE_{\pi_\theta}[r(s_{t\prime}, a_{t\prime})|s_t,a_t]$ ：在$s_t$中采用动作$a_t$得到的奖励总和。
$V^\pi (s_t) = E_{a_t\sim\pi_\theta(a_t|s_t)}[Q^\pi(s_t, a_t)]$：从$s_t$中得到的奖励总和。
$A^\pi(s_t,a_t) = Q^\pi(s_t,a_t) - V^\pi(s_t)$：评价$a_t$的优秀的程度。
有了以上的定义，我们就可以得到策略的总的奖励。
$$J(\theta) \approx \frac{1}{N}\sum_{t=1}^N\sum_{t=1}^T\nabla_{\theta}\log\pi_{\theta}(a_{i,t}|s_{i,t})A^\pi(s_{i,t}|a_i,t)$$
其中$A^\pi$越好，则方差越小。

## value function fitting

有了上述定义的三个策略(Q, V, A)，就可以进行对应函数的拟合了。那么问题就是，**把什么拟合成什么**？这其实就是`actor-critic`算法。
$$Q^{\pi}(s_t, a_t) \approx r(s_t,a_t) + V^\pi(s_t,a_t)$$ $$A^\pi(s_t,a_t) \approx r(s_t,a_t) + V^\pi(s_t,a_t) - V^\pi(s_t)$$
从以上等式可以看到，actor-critic算法基本上是先拟合值函数(value function)，随后再对Q或A进行拟合。
本小节只讨论*value function*的拟合。
我们使用神经网络进行拟合。输入状态$s$，得到$V^\pi$的估计值。
现在，我们改写$J(\theta)$，得到$$J(\theta) = E_{s_1\sim p(s_1)}[V^\pi(s_1)]$$

有了上述的理论后，就该开始思考，**如何执行策略估计呢**？
答案是使用`Monte Carlo policy evaluation`，这其实也是策略梯度在做的事情。这种方法指的是该状态的值函数大约等于你从该状态开始执行该策略能得到的总共的奖励。
$$V^\pi(s_t) \approx \sum_{t\prime = t}^Tr(s_{t\prime}, a_{t\prime})$$
如果多次采样，能够使得估计的效果更好。
$$V^\pi(s_t) \approx \frac{1}{N}\sum_{i=1}^N\sum_{t\prime=t}^Tr(s_{t\prime}, a_{t\prime})$$
但这种多次采样只是理论上的方法，事实上很难实现。

## Monte Carlo evaluation with function approximation

虽然多次采样并不能实现，但是如果使用的是函数逼近去拟合值函数$V^\pi$，就算无法回到之前的状态，我们仍然可以获得更低的方差。相比于多次采样的估计，虽然效果比不上，但是结果仍然很不错。

有了前面的这些铺垫，我们就可以回答“把什么拟合成什么”的问题。我们使用NN去拟合状态$S$，输出的是$V^\pi$的估计值。
我们会生成一堆轨迹，对于每一个轨迹的每一个时间步，我们把当前的状态当成输入放进训练集，同时也放入剩余的总奖励当作输出。这样就有了一堆状态和奖励组成的训练集。
下面给出一个新的缩写:
$$y_{i,t} = \sum_{t\prime=t}^Tr(s_{i,t\prime},a_{i,t\prime})$$
表示来自第$i$条轨迹的时间步$t$下的状态$s_{i,t}$，能在该条轨迹下获得的总的剩余奖励。
